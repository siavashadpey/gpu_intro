{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cs179_lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNKhJRGD9Up6BiOjrkVgcU4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siavashadpey/gpu_intro/blob/master/cs179_lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_NrbmMiJ4sj",
        "colab_type": "text"
      },
      "source": [
        "Lab 2 of CS179 (http://courses.cms.caltech.edu/cs179/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EWYobr2JyP-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "eef16872-d9ee-45c6-8c25-f5e79f689cd3"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-gl6srsfo\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-gl6srsfo\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4307 sha256=8936d1d622f03daa9664a873352a6bd4f15b7149dccc180bbd7cc37d710703f9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rgw5azou/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gi2MpmsZJ8Qz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "309e364a-bfde-4a7a-98cc-2abeafb0a847"
      },
      "source": [
        "%%cu\n",
        "\n",
        "#include <cassert>\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cstring>\n",
        "#include <string>\n",
        "#include <unistd.h> // sleep, fork, getpid\n",
        "#include <signal.h> // kill\n",
        "#include <cstdio> // printf\n",
        "#include <stdlib.h> // popen, pclose, atoi, fread\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "enum TransposeImplementation { NAIVE, SHMEM, OPTIMAL };\n",
        "\n",
        "/*\n",
        " =============================== STR: DEVICE    ================================\n",
        "*/\n",
        "/*\n",
        " * TODO for all kernels (including naive):\n",
        " * Leave a comment above all non-coalesced memory accesses and bank conflicts.\n",
        " * Make it clear if the suboptimal access is a read or write. If an access is\n",
        " * non-coalesced, specify how many cache lines it touches, and if an access\n",
        " * causes bank conflicts, say if its a 2-way bank conflict, 4-way bank\n",
        " * conflict, etc.\n",
        " *\n",
        " * Comment all of your kernels.\n",
        " */\n",
        "\n",
        "/*\n",
        " * Each block of the naive transpose handles a 64x64 block of the input matrix,\n",
        " * with each thread of the block handling a 1x4 section and each warp handling\n",
        " * a 32x4 section.\n",
        " *\n",
        " * If we split the 64x64 matrix into 32 blocks of shape (32, 4), then we have\n",
        " * a block matrix of shape (2 blocks, 16 blocks).\n",
        " * Warp 0 handles block (0, 0), warp 1 handles (1, 0), warp 2 handles (0, 1),\n",
        " * warp n handles block (n % 2, n / 2).\n",
        " *\n",
        " * This kernel is launched with block shape (64, 16) and grid shape\n",
        " * (n / 64, n / 64) where n is the size of the square matrix.\n",
        " *\n",
        " * You may notice that we suggested in lecture that threads should be able to\n",
        " * handle an arbitrary number of elements and that this kernel handles exactly\n",
        " * 4 elements per thread. This is OK here because to overwhelm this kernel\n",
        " * it would take a 4194304 x 4194304 matrix, which would take ~17.6TB of\n",
        " * memory (well beyond what I expect GPUs to have in the next few years).\n",
        " */\n",
        "__global__\n",
        "void naiveTransposeKernel(const float *input, float *output, int n) {\n",
        "    // DONE: do not modify code, just comment on suboptimal accesses\n",
        "\n",
        "    const int i = threadIdx.x + 64 * blockIdx.x;\n",
        "    int j = 4 * threadIdx.y + 64 * blockIdx.y;\n",
        "    const int end_j = j + 4;\n",
        "\n",
        "    // COMMENT: we can unroll j to allow for ILP\n",
        "    // COMMENT: memory access to ouput is not coallesced (stride = n, not 1).\n",
        "    //          it touches 32 cache lines (assuming n > 32).\n",
        "    for (; j < end_j; j++)\n",
        "        output[j + n * i] = input[i + n * j];\n",
        "}\n",
        "\n",
        "__global__\n",
        "void shmemTransposeKernel(const float *input, float *output, int n) {\n",
        "    // DONE: Modify transpose kernel to use shared memory. All global memory\n",
        "    // reads and writes should be coalesced. Minimize the number of shared\n",
        "    // memory bank conflicts (0 bank conflicts should be possible using\n",
        "    // padding). Again, comment on all sub-optimal accesses.\n",
        "\n",
        "    // each block takes care of a chunk of size 64 x 64\n",
        "    __shared__ float data[64][65]; // 64 x 64 block with an extra column of padding\n",
        "\n",
        "    // blockDim.x = 4*blockDim.y = 64\n",
        "    int i = threadIdx.x + 64 * blockIdx.x;\n",
        "    int j = 4 * threadIdx.y + 64 * blockIdx.y;\n",
        "    int end_j = j + 4;\n",
        "    \n",
        "    // copy input to shared memory\n",
        "    const int li = threadIdx.x;\n",
        "    int lj = 4*threadIdx.y;\n",
        "    for (; j < end_j; j++, lj++) {\n",
        "        // we're using a stride of 1 for global memory --> memory coallescing\n",
        "        data[lj][li] = input[i + j * n]; \n",
        "        // note: even without the padding, no bank conflicts would occur here\n",
        "        // elements in data[lj][:] belong to different banks\n",
        "    }\n",
        "\n",
        "    __syncthreads(); // sync with all other threads in current block\n",
        "\n",
        "    // reset some indices\n",
        "    i = threadIdx.x + 64 * blockIdx.y;\n",
        "    j = 4 * threadIdx.y + 64 * blockIdx.x;\n",
        "    lj = 4*threadIdx.y;\n",
        "    end_j = j + 4;\n",
        "    // copy output to global memory\n",
        "    for (; j < end_j; j++, lj++) {\n",
        "        // we're using a stride of 1 for global memory --> memory coallescing\n",
        "        output[i + j * n] = data[li][lj];\n",
        "        // the padding is necesseray to prevent bank conflicts here\n",
        "        // each element in data[:][lj] belongs to different banks\n",
        "    }\n",
        "    // COMMENT: for loops should be unrolled for ILP\n",
        "}\n",
        "\n",
        "__global__\n",
        "void optimalTransposeKernel(const float *input, float *output, int n) {\n",
        "    // TODO: This should be based off of your shmemTransposeKernel.\n",
        "    // Use any optimization tricks discussed so far to improve performance.\n",
        "    // Consider ILP and loop unrolling.\n",
        "\n",
        "__shared__ float data[64][65]; // 64 x 64 block with an extra column of padding\n",
        "\n",
        "    // blockDim.x = 4*blockDim.y = 64\n",
        "    int i = threadIdx.x + 64 * blockIdx.x;\n",
        "    int j = 4 * threadIdx.y + 64 * blockIdx.y;\n",
        "    \n",
        "    // copy input to shared memory\n",
        "    const int li = threadIdx.x;\n",
        "    const int lj = 4*threadIdx.y;\n",
        "    // we've manually unrolled the for loop\n",
        "    data[lj    ][li] = input[i +  j      * n]; \n",
        "    data[lj + 1][li] = input[i + (j + 1) * n]; \n",
        "    data[lj + 2][li] = input[i + (j + 2) * n]; \n",
        "    data[lj + 3][li] = input[i + (j + 3) * n]; \n",
        "\n",
        "    __syncthreads(); // sync with all other threads in current block\n",
        "\n",
        "    // reset some indices\n",
        "    i = threadIdx.x + 64 * blockIdx.y;\n",
        "    j = 4 * threadIdx.y + 64 * blockIdx.x;\n",
        "\n",
        "    // copy output to global memory\n",
        "    // we've manually unrolled the for loop\n",
        "    output[i + j       * n] = data[li][lj    ];\n",
        "    output[i + (j + 1) * n] = data[li][lj + 1];\n",
        "    output[i + (j + 2) * n] = data[li][lj + 2];\n",
        "    output[i + (j + 3) * n] = data[li][lj + 3];\n",
        "}\n",
        "\n",
        "void cudaTranspose(\n",
        "    const float *d_input,\n",
        "    float *d_output,\n",
        "    int n,\n",
        "    TransposeImplementation type)\n",
        "{\n",
        "    if (type == NAIVE) {\n",
        "        dim3 blockSize(64, 16);\n",
        "        dim3 gridSize(n / 64, n / 64);\n",
        "        naiveTransposeKernel<<<gridSize, blockSize>>>(d_input, d_output, n);\n",
        "    }\n",
        "    else if (type == SHMEM) {\n",
        "        dim3 blockSize(64, 16);\n",
        "        dim3 gridSize(n / 64, n / 64);\n",
        "        shmemTransposeKernel<<<gridSize, blockSize>>>(d_input, d_output, n);\n",
        "    }\n",
        "    else if (type == OPTIMAL) {\n",
        "        dim3 blockSize(64, 16);\n",
        "        dim3 gridSize(n / 64, n / 64);\n",
        "        optimalTransposeKernel<<<gridSize, blockSize>>>(d_input, d_output, n);\n",
        "    }\n",
        "    // Unknown type\n",
        "    else\n",
        "        assert(false);\n",
        "}\n",
        "\n",
        "/*\n",
        " =============================== END: DEVICE    ================================\n",
        "*/\n",
        "\n",
        "/*\n",
        " =============================== STR: UTILITIES ================================\n",
        "*/\n",
        "\n",
        "namespace TA_Utilities\n",
        "{\n",
        "  /* Select the least utilized GPU on this system. Estimate\n",
        "     GPU utilization using GPU temperature. UNIX only. */\n",
        "  void select_coldest_GPU() \n",
        "  {\n",
        "      // Get the number of GPUs on this machine\n",
        "      int num_devices;\n",
        "      cudaGetDeviceCount(&num_devices);\n",
        "      if(num_devices == 0) {\n",
        "          printf(\"select_coldest_GPU: Error - No GPU detected\\n\");\n",
        "          return;\n",
        "      }\n",
        "      // Read GPU info into buffer \"output\"\n",
        "      const unsigned int MAX_BYTES = 10000;\n",
        "      char output[MAX_BYTES];\n",
        "      FILE *fp = popen(\"nvidia-smi &> /dev/null\", \"r\");\n",
        "      size_t bytes_read = fread(output, sizeof(char), MAX_BYTES, fp);\n",
        "      pclose(fp);\n",
        "      if(bytes_read == 0) {\n",
        "          printf(\"Error - No Temperature could be read\\n\");\n",
        "          return;\n",
        "\t  }\n",
        "      // array to hold GPU temperatures\n",
        "      int * temperatures = new int[num_devices];\n",
        "      // parse output for temperatures using knowledge of \"nvidia-smi\" output format\n",
        "      int i = 0;\n",
        "      unsigned int num_temps_parsed = 0;\n",
        "      while(output[i] != '\\0') {\n",
        "          if(output[i] == '%') {\n",
        "              unsigned int temp_begin = i + 1;\n",
        "              while(output[i] != 'C') {\n",
        "                  ++i;\n",
        "              }\n",
        "              unsigned int temp_end = i;\n",
        "              char this_temperature[32];\n",
        "              // Read in the characters cooresponding to this temperature\n",
        "              for(unsigned int j = 0; j < temp_end - temp_begin; ++j) {\n",
        "                  this_temperature[j] = output[temp_begin + j];\n",
        "              }\n",
        "              this_temperature[temp_end - temp_begin + 1] = '\\0';\n",
        "              // Convert string representation to int\n",
        "              temperatures[num_temps_parsed] = atoi(this_temperature);\n",
        "              num_temps_parsed++;\n",
        "          }\n",
        "          ++i;\n",
        "      }\n",
        "      // Get GPU with lowest temperature\n",
        "      int min_temp = 1e7, index_of_min = -1;\n",
        "      for (int i = 0; i < num_devices; i++) \n",
        "      {\n",
        "          int candidate_min = temperatures[i];\n",
        "          if(candidate_min < min_temp) \n",
        "          {\n",
        "              min_temp = candidate_min;\n",
        "              index_of_min = i;\n",
        "          }\n",
        "      }\n",
        "      // Tell CUDA to use the GPU with the lowest temeprature\n",
        "      printf(\"Index of the GPU with the lowest temperature: %d (%d C)\\n\", \n",
        "          index_of_min, min_temp);\n",
        "      cudaSetDevice(index_of_min);\n",
        "      // Free memory and return\n",
        "      delete(temperatures);\n",
        "      return;\n",
        "  } // end \"void select_coldest_GPU()\"\"\n",
        "\n",
        "  /* Create a child thread that will kill the parent thread after the\n",
        "     specified time limit has been exceeded */\n",
        "  void enforce_time_limit(int time_limit) {\n",
        "      printf(\"Time limit for this program set to %d seconds\\n\", time_limit);\n",
        "      int parent_id = getpid();\n",
        "      pid_t child_id = fork();\n",
        "      // The fork call creates a lignering child thread that will \n",
        "      // kill the parent thread after the time limit has exceeded\n",
        "      // If it hasn't already terminated.\n",
        "      if(child_id == 0) // \"I am the child thread\"\n",
        "      {\n",
        "          sleep(time_limit);\n",
        "          if( kill(parent_id, SIGTERM) == 0) {\n",
        "              printf(\"enforce_time_limit.c: Program terminated\"\n",
        "               \" for taking longer than %d seconds\\n\", time_limit);\n",
        "          }\n",
        "          // Ensure that parent was actually terminated\n",
        "          sleep(2);\n",
        "          if( kill(parent_id, SIGKILL) == 0) {\n",
        "              printf(\"enforce_time_limit.c: Program terminated\"\n",
        "               \" for taking longer than %d seconds\\n\", time_limit);\n",
        "          } \n",
        "          // Child thread has done its job. Terminate now.\n",
        "          exit(0);\n",
        "      }\n",
        "      else // \"I am the parent thread\"\n",
        "      {\n",
        "          // Allow the parent thread to continue doing what it was doing\n",
        "          return;\n",
        "      }\n",
        "  } // end \"void enforce_time_limit(int time_limit)\n",
        "\n",
        "\n",
        "} // end \"namespace TA_Utilities\"\n",
        "\n",
        "/*\n",
        " =============================== END: UTILITIES ================================\n",
        "*/\n",
        "\n",
        "/*\n",
        " =============================== STR: HOST      ================================\n",
        "*/\n",
        "/*\n",
        " * NOTE: You can use this macro to easily check cuda error codes\n",
        " * and get more information.\n",
        " * \n",
        " * Modified from:\n",
        " * http://stackoverflow.com/questions/14038589/\n",
        " *         what-is-the-canonical-way-to-check-for-errors-using-the-cuda-runtime-api\n",
        " */\n",
        "#define gpuErrChk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line,\n",
        "    bool abort = true)\n",
        "{\n",
        "    if (code != cudaSuccess) {\n",
        "        fprintf(stderr,\"GPUassert: %s %s %d\\n\",\n",
        "            cudaGetErrorString(code), file, line);\n",
        "        exit(code);\n",
        "    }\n",
        "}\n",
        "\n",
        "/* a and b point to n x n matrices. This method checks that A = B^T. */\n",
        "void checkTransposed(const float *a, const float *b, int n) {\n",
        "    bool correct = true;\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < n; j++) {\n",
        "            if (a[i + n * j] != b[j + n * i]) {\n",
        "                correct = false;\n",
        "                fprintf(stderr,\n",
        "                    \"Transpose failed: a[%d, %d] != b[%d, %d], %f != %f\\n\",\n",
        "                    i, j, j, i, a[i + n * j], b[j + n * i]);\n",
        "                assert(correct);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    assert(correct);\n",
        "}\n",
        "\n",
        "/* Naive CPU transpose, takes an n x n matrix in input and writes to output. */\n",
        "void cpuTranspose(const float *input, float *output, int n) {\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < n; j++) {\n",
        "            output[j + n * i] = input[i + n * j];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        " * Fills fill with random numbers is [0, 1]. Size is number of elements to\n",
        " * assign.\n",
        " */\n",
        "void randomFill(float *fill, int size) {\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        float r = static_cast<float>(rand()) / static_cast<float>(RAND_MAX);\n",
        "        fill[i] = r;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "\n",
        "    // These functions allow you to select the least utilized GPU \n",
        "    // on your system as well as enforce a time limit on program execution.\n",
        "    // Please leave these enabled as a courtesy to your fellow classmates\n",
        "    // if you are using a shared computer. You may ignore or remove these \n",
        "    // functions if you are running on your local machine.\n",
        "    TA_Utilities::select_coldest_GPU();\n",
        "    int max_time_allowed_in_seconds = 10;\n",
        "    TA_Utilities::enforce_time_limit(max_time_allowed_in_seconds);\n",
        "    \n",
        "    // Seed random number generator\n",
        "    srand(2016);\n",
        "\n",
        "    std::string kernel = \"all\";\n",
        "    int size_to_run = -1;\n",
        "\n",
        "    // Check arguments\n",
        "    assert(argc <= 3);\n",
        "    if (argc >= 2)\n",
        "        size_to_run = atoi(argv[1]);\n",
        "    if (argc == 3)\n",
        "        kernel = argv[2];\n",
        "\n",
        "    if (!(size_to_run == -1  ||\n",
        "         size_to_run == 512  ||\n",
        "         size_to_run == 1024 ||\n",
        "         size_to_run == 2048 ||\n",
        "         size_to_run == 4096))\n",
        "    {\n",
        "        fprintf(stderr,\n",
        "            \"Program only designed to run sizes 512, 1024, 2048, 4096\\n\");\n",
        "    }\n",
        "\n",
        "    assert(kernel == \"all\"     ||\n",
        "        kernel == \"cpu\"        ||\n",
        "        kernel == \"gpu_memcpy\" ||\n",
        "        kernel == \"naive\"      ||\n",
        "        kernel == \"shmem\"      ||\n",
        "        kernel == \"optimal\");\n",
        "\n",
        "    // Run the transpose implementations for all desired sizes (2^9 = 512, \n",
        "    // 2^12 = 4096)\n",
        "    for (int _i = 9; _i < 13; _i++) {\n",
        "        int n = 1 << _i;\n",
        "        if (size_to_run != -1 && size_to_run != n)\n",
        "            continue;\n",
        "\n",
        "        assert(n % 64 == 0);\n",
        "\n",
        "        cudaEvent_t start;\n",
        "        cudaEvent_t stop;\n",
        "\n",
        "#define START_TIMER() {                                                        \\\n",
        "            gpuErrChk(cudaEventCreate(&start));                                \\\n",
        "            gpuErrChk(cudaEventCreate(&stop));                                 \\\n",
        "            gpuErrChk(cudaEventRecord(start));                                 \\\n",
        "        }\n",
        "\n",
        "#define STOP_RECORD_TIMER(name) {                                              \\\n",
        "            gpuErrChk(cudaEventRecord(stop));                                  \\\n",
        "            gpuErrChk(cudaEventSynchronize(stop));                             \\\n",
        "            gpuErrChk(cudaEventElapsedTime(&name, start, stop));               \\\n",
        "            gpuErrChk(cudaEventDestroy(start));                                \\\n",
        "            gpuErrChk(cudaEventDestroy(stop));                                 \\\n",
        "        }\n",
        "\n",
        "        // Initialize timers\n",
        "        float cpu_ms = -1;\n",
        "        float gpu_memcpy = -1;\n",
        "        float naive_gpu_ms = -1;\n",
        "        float shmem_gpu_ms = -1;\n",
        "        float optimal_gpu_ms = -1;\n",
        "\n",
        "        // Allocate host memory\n",
        "        float *input = new float[n * n];\n",
        "        float *output = new float[n * n];\n",
        "\n",
        "        // Allocate device memory\n",
        "        float *d_input;\n",
        "        float *d_output;\n",
        "        gpuErrChk(cudaMalloc(&d_input, n * n * sizeof(float)));\n",
        "        gpuErrChk(cudaMalloc(&d_output, n * n * sizeof(float)));\n",
        "\n",
        "        // Initialize input data to random numbers in [0, 1]\n",
        "        randomFill(input, n * n);\n",
        "\n",
        "        // Copy input to GPU\n",
        "        gpuErrChk(cudaMemcpy(d_input, input, n * n * sizeof(float), \n",
        "            cudaMemcpyHostToDevice));\n",
        "\n",
        "\n",
        "        // CPU implementation\n",
        "        if (kernel == \"cpu\" || kernel == \"all\") {\n",
        "            START_TIMER();\n",
        "            cpuTranspose(input, output, n);\n",
        "            STOP_RECORD_TIMER(cpu_ms);\n",
        "\n",
        "            checkTransposed(input, output, n);\n",
        "            memset(output, 0, n * n * sizeof(float));\n",
        "\n",
        "            printf(\"Size %d naive CPU: %f ms\\n\", n, cpu_ms);\n",
        "        }\n",
        "\n",
        "        // GPU memcpy (useful for comparison purposes)\n",
        "        if (kernel == \"gpu_memcpy\" || kernel == \"all\") {\n",
        "            START_TIMER();\n",
        "            gpuErrChk(cudaMemcpy(d_output, d_input, n * n * sizeof(float),\n",
        "                cudaMemcpyDeviceToDevice));\n",
        "            STOP_RECORD_TIMER(gpu_memcpy);\n",
        "\n",
        "            gpuErrChk(cudaMemset(d_output, 0, n * n * sizeof(float)));\n",
        "            printf(\"Size %d GPU memcpy: %f ms\\n\", n, gpu_memcpy);\n",
        "        }\n",
        "\n",
        "        // Naive GPU implementation\n",
        "        if (kernel == \"naive\" || kernel == \"all\") {\n",
        "            START_TIMER();\n",
        "            cudaTranspose(d_input, d_output, n, NAIVE);\n",
        "            STOP_RECORD_TIMER(naive_gpu_ms);\n",
        "\n",
        "            gpuErrChk(cudaMemcpy(output, d_output, n * n * sizeof(float), \n",
        "                cudaMemcpyDeviceToHost));\n",
        "            checkTransposed(input, output, n);\n",
        "\n",
        "            memset(output, 0, n * n * sizeof(float));\n",
        "            gpuErrChk(cudaMemset(d_output, 0, n * n * sizeof(float)));\n",
        "\n",
        "            printf(\"Size %d naive GPU: %f ms\\n\", n, naive_gpu_ms);\n",
        "        }\n",
        "\n",
        "        // shmem GPU implementation\n",
        "        if (kernel == \"shmem\" || kernel == \"all\") {\n",
        "            START_TIMER();\n",
        "            cudaTranspose(d_input, d_output, n, SHMEM);\n",
        "            STOP_RECORD_TIMER(shmem_gpu_ms);\n",
        "\n",
        "            gpuErrChk(cudaMemcpy(output, d_output, n * n * sizeof(float), \n",
        "                cudaMemcpyDeviceToHost));\n",
        "            checkTransposed(input, output, n);\n",
        "\n",
        "            memset(output, 0, n * n * sizeof(float));\n",
        "            gpuErrChk(cudaMemset(d_output, 0, n * n * sizeof(float)));\n",
        "\n",
        "            printf(\"Size %d shmem GPU: %f ms\\n\", n, shmem_gpu_ms);\n",
        "        }\n",
        "\n",
        "        // Optimal GPU implementation\n",
        "        if (kernel == \"optimal\"    || kernel == \"all\") {\n",
        "            START_TIMER();\n",
        "            cudaTranspose(d_input, d_output, n, OPTIMAL);\n",
        "            STOP_RECORD_TIMER(optimal_gpu_ms);\n",
        "\n",
        "            gpuErrChk(cudaMemcpy(output, d_output, n * n * sizeof(float), \n",
        "                cudaMemcpyDeviceToHost));\n",
        "            checkTransposed(input, output, n);\n",
        "\n",
        "            memset(output, 0, n * n * sizeof(float));\n",
        "            gpuErrChk(cudaMemset(d_output, 0, n * n * sizeof(float)));\n",
        "\n",
        "            printf(\"Size %d optimal GPU: %f ms\\n\", n, optimal_gpu_ms);\n",
        "        }\n",
        "\n",
        "        // Free host memory\n",
        "        delete[] input;\n",
        "        delete[] output;\n",
        "\n",
        "        // Free device memory\n",
        "        gpuErrChk(cudaFree(d_input));\n",
        "        gpuErrChk(cudaFree(d_output));\n",
        "\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n",
        "\n",
        "/*\n",
        " =============================== END: HOST      ================================\n",
        "*/\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index of the GPU with the lowest temperature: 0 (0 C)\n",
            "Time limit for this program set to 10 seconds\n",
            "Size 512 naive CPU: 1.901312 ms\n",
            "Size 512 GPU memcpy: 0.042560 ms\n",
            "Size 512 naive GPU: 0.135296 ms\n",
            "Size 512 shmem GPU: 0.032160 ms\n",
            "Size 512 optimal GPU: 0.026112 ms\n",
            "\n",
            "Size 1024 naive CPU: 7.437024 ms\n",
            "Size 1024 GPU memcpy: 0.090400 ms\n",
            "Size 1024 naive GPU: 0.195040 ms\n",
            "Size 1024 shmem GPU: 0.088064 ms\n",
            "Size 1024 optimal GPU: 0.070944 ms\n",
            "\n",
            "Size 2048 naive CPU: 38.028671 ms\n",
            "Size 2048 GPU memcpy: 0.297824 ms\n",
            "Size 2048 naive GPU: 0.735008 ms\n",
            "Size 2048 shmem GPU: 0.281696 ms\n",
            "Size 2048 optimal GPU: 0.259936 ms\n",
            "\n",
            "Size 4096 naive CPU: 239.744156 ms\n",
            "Size 4096 GPU memcpy: 1.064576 ms\n",
            "Size 4096 naive GPU: 2.878432 ms\n",
            "Size 4096 shmem GPU: 1.092800 ms\n",
            "Size 4096 optimal GPU: 1.018912 ms\n",
            "\n",
            "Index of the GPU with the lowest temperature: 0 (0 C)\n",
            "Time limit for this program set to 10 seconds\n",
            "enforce_time_limit.c: Program terminated for taking longer than 10 seconds\n",
            "enforce_time_limit.c: Program terminated for taking longer than 10 seconds\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}