CS 179: GPU Computing
Lab 1: Introduction to CUDA
Name:

--------------------------------------------------------------------------------
Question 1.1: Latency Hiding (5 points)
--------------------------------------------------------------------------------
Approximately how many arithmetic instructions does it take to hide the latency
of a single arithmetic instruction on a GK110?

Assume all of the arithmetic instructions are independent (ie have no
instruction dependencies).

You do not need to consider the number of execution cores on the chip.

Hint: What is the latency of an arithmetic instruction? How many instructions
can a GK110 begin issuing in 1 clock cycle (assuming no dependencies)?

The latency of an arithmetic instruction is 10 ns. The number of instructions 
per cycle for a GK110 is 8 (4 warps each clock, up to 2 instructions in each warp). 
A GPU clock is 1 GHz (1 clock per ns).

Thus, to fully hide the 10 ns latency of an arithmetic instruction, 8*10=80 instructions
must be issued.

This is an example of how ILP (Instruction Level Paralllelism) can hide latency
similar to high occupancy.

--------------------------------------------------------------------------------
Question 1.2: Thread Divergence (6 points)
--------------------------------------------------------------------------------
Let the block shape be (32, 32, 1).

(a)
int idx = threadIdx.y + blockSize.y * threadIdx.x;
if (idx % 32 < 16) // %: remainder operator
    foo();
else
    bar();

Does this code diverge? Why or why not?

No.

idx % 32 = (threadIdx.y + 32 * threadIdx.x) % 32 = threadIdx.y.
Since each warp is of size 32 threads and BlockDim.x is 32 too, then
threadIdx.y is constant for each warp, which means that ALL the threads in a warp
either executes foo() or bar().


(b)
const float pi = 3.14;
float result = 1.0;
for (int i = 0; i < threadIdx.x; i++)
    result *= pi;

Does this code diverge? Why or why not? (This is a bit of a trick question,
either "yes" or "no can be a correct answer with appropriate explanation.)

Yes.
For instance, for i=30, threadIdx.x=31 executes result*=pi, where as
threads in the warp (0 to 30) are masked. However, this thread divergence
does not deteriorate performance because the conditional statement does 
not cause multi-branches (the execution time is the same as if no thread 
divergence occured). So we can also argue that there is no thread divergence.

--------------------------------------------------------------------------------
Question 1.3: Coalesced Memory Access (9 points)
--------------------------------------------------------------------------------
Let the block shape be (32, 32, 1). Let data be a (float *) pointing to global
memory and let data be 128 byte aligned (so data % 128 == 0).

Consider each of the following access patterns.

(a)
data[threadIdx.x + blockSize.x * threadIdx.y] = 1.0;

Is this write coalesced? How many 128 byte cache lines does this write to?

Yes. Each warp write to 1 128 (4*32) byte cache line

(b)
data[threadIdx.y + blockSize.y * threadIdx.x] = 1.0;

Is this write coalesced? How many 128 byte cache lines does this write to?

No. The stride is blockSize.y=32. The stride should be 1 for memory coalescing.
Each warp writes to 32 128 byte cache lines.

(c)
data[1 + threadIdx.x + blockSize.x * threadIdx.y] = 1.0;

Is this write coalesced? How many 128 byte cache lines does this write to?

No. Although the stride is 1, the 1 in the index offsets everything (e.g. last thread
will access a different cache line than the other bites)
Each warp writes to 2 128 byte cache line

--------------------------------------------------------------------------------
Question 1.4: Bank Conflicts and Instruction Dependencies (15 points)
--------------------------------------------------------------------------------
Let's consider multiplying a 32 x 128 matrix with a 128 x 32 element matrix.
This outputs a 32 x 32 matrix. We'll use 32 ** 2 = 1024 threads and each thread
will compute 1 output element. Although its not optimal, for the sake of
simplicity let's use a single block, so grid shape = (1, 1, 1),
block shape = (32, 32, 1).

For the sake of this problem, let's assume both the left and right matrices have
already been stored in shared memory in column major format. This means the
element in the ith row and jth column is accessible at lhs[i + 32 * j] for the
left hand side and rhs[i + 128 * j] for the right hand side.

This kernel will write to a variable called output stored in shared memory.

Consider the following kernel code:

int i = threadIdx.x;
int j = threadIdx.y;
for (int k = 0; k < 128; k += 2) {
    output[i + 32 * j] += lhs[i + 32 * k] * rhs[k + 128 * j];
    output[i + 32 * j] += lhs[i + 32 * (k + 1)] * rhs[(k + 1) + 128 * j];
}

(a)
Are there bank conflicts in this code? If so, how many ways is the bank conflict
(2-way, 4-way, etc)?

No.
Note that for each warp, j is constant since blockDim.x=32.
Since all threads in the warp execute the same instruction, k is also contant.
So we just need to worry about i which ranges from 0 to 31. 
Since lhs is stored in column-major format, its entries
are stored as follows in the banks (where lhs(i,k) = lhs[i + 32*k]):

Banks   Data
-----   --------------------
0       lhs[0] | lhs[32]
1       lhs[1] | lhs[33]
2       lhs[2] | lhs[34]
.       .      | .      
.       .      | .
.       .      | .
31      lhs[31]| lhs[63]

We can see that different threads access different banks, so no bank conflicts occur.

(b)
Expand the inner part of the loop (below)

output[i + 32 * j] += lhs[i + 32 * k] * rhs[k + 128 * j];
output[i + 32 * j] += lhs[i + 32 * (k + 1)] * rhs[(k + 1) + 128 * j];

into "psuedo-assembly" as was done in the coordinate addition example in lecture
4.

There's no need to expand the indexing math, only to expand the loads, stores,
and math. Notably, the operation a += b * c can be computed by a single
instruction called a fused multiply add (FMA), so this can be a single
instruction in your "psuedo-assembly".

Hint: Each line should expand to 5 instructions.

1) a0 = lhs[i + 32 * k]
2) b0 = rhs[k + 128 * j]
3) c0 = ouput[i + 32 * j]
// Fused multiply-add. It's performed in one operation. 
// Faster than multiplying then adding
4) d0 = FMA(a0, b0, c0) 
5) output[i + 32 * j] = d0

6) a1 = lhs[i + 32 * (k + 1)]
7) b1 = rhs[(k + 1) + 128 * j]
8) c1 = ouput[i + 32 * j]
9) d1 = FMA(a1, b1, c1) 
10) output[i + 32 * j] = d1

(c)
4) depends on 1), 2), 3) and 5) depends on 4)
Similarly, 9) depends on 6), 7), 8) and 10) depends on 9)
8) also depends on 5)

(d)
Rewrite the code given at the beginning of this problem to minimize instruction
dependencies. You can add or delete instructions (deleting an instruction is a
valid way to get rid of a dependency!) but each iteration of the loop must still
process 2 values of k.

output[i + 32 * j] += lhs[i + 32 * k] * rhs[k + 128 * j];
float o1;
o1 = lhs[i + 32 * (k + 1)] * rhs[(k + 1) + 128 * j];
output[i + 32 * j] += o1;

This removes the dependency of 8) on 5)

Pseudo-assembly:
1) a0 = lhs[i + 32 * k]
2) b0 = rhs[k + 128 * j]
3) c0 = ouput[i + 32 * j]
4) d0 = FMA(a0, b0, c0) 
5) output[i + 32 * j] = d0
6) a1 = lhs[i + 32 * (k + 1)]
7) b1 = rhs[(k + 1) + 128 * j]
8) initialize o1;
9) o1 = a1*b1;
10) output[i + 32 * j] += o1

while 1) to 5) are being computed, 6) to 9) can be computed concurrently too

(e)
Can you think of any other anything else you can do that might make this code
run faster?

We can have a larger unrolled for loop than 2 to further increase the ILP.

--------------------------------------------------------------------------------
PART 2 - Matrix transpose optimization (65 points)
--------------------------------------------------------------------------------
Optimize the CUDA matrix transpose implementations in transpose_cuda.cu. Read
ALL of the TODO comments. Matrix transpose is a common exercise in GPU
optimization, so do not search for existing GPU matrix transpose code on the
internet.

Your transpose code only need to be able to transpose square matrices where the
side length is a multiple of 64.

The initial implementation has each block of 1024 threads handle a 64x64 block
of the matrix, but you can change anything about the kernel if it helps obtain
better performance.

The main method of transpose.cc already checks for correctness for all transpose
results, so there should be an assertion failure if your kernel produces incorrect
output.

The purpose of the shmemTransposeKernel is to demonstrate proper usage of global
and shared memory. The optimalTransposeKernel should be built on top of
shmemTransposeKernel and should incorporate any "tricks" such as ILP, loop
unrolling, vectorized IO, etc that have been discussed in class.

You can compile and run the code by running

make transpose
./transpose

and the build process was tested on minuteman. If this does not work on haru for
you, be sure to add the lines

export PATH=/usr/local/cuda-6.5/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-6.5/lib64:$LD_LIBRARY_PATH

to your ~/.profile file (and then exit and ssh back in to restart your shell).

On OS X, you may have to run or add to your .bash_profile the command

export DYLD_LIBRARY_PATH=$DYLD_LIBRARY_PATH:/usr/local/cuda/lib/

in order to get dynamic library linkage to work correctly.

The transpose program takes 2 optional arguments: input size and method. Input
size must be one of -1, 512, 1024, 2048, 4096, and method must be one all,
cpu, gpu_memcpy, naive, shmem, optimal. Input size is the first argument and
defaults to -1. Method is the second argument and defaults to all. You can pass
input size without passing method, but you cannot pass method without passing an
input size.

Examples:
./transpose
./transpose 512
./transpose 4096 naive
./transpose -1 optimal

Copy paste the output of ./transpose.cc into README.txt once you are done.
Describe the strategies used for performance in either block comments over the
kernel (as done for naiveTransposeKernel) or in README.txt.

Size 512 naive CPU: 1.901312 ms
Size 512 GPU memcpy: 0.042560 ms
Size 512 naive GPU: 0.135296 ms
Size 512 shmem GPU: 0.032160 ms
Size 512 optimal GPU: 0.026112 ms

Size 1024 naive CPU: 7.437024 ms
Size 1024 GPU memcpy: 0.090400 ms
Size 1024 naive GPU: 0.195040 ms
Size 1024 shmem GPU: 0.088064 ms
Size 1024 optimal GPU: 0.070944 ms

Size 2048 naive CPU: 38.028671 ms
Size 2048 GPU memcpy: 0.297824 ms
Size 2048 naive GPU: 0.735008 ms
Size 2048 shmem GPU: 0.281696 ms
Size 2048 optimal GPU: 0.259936 ms

Size 4096 naive CPU: 239.744156 ms
Size 4096 GPU memcpy: 1.064576 ms
Size 4096 naive GPU: 2.878432 ms
Size 4096 shmem GPU: 1.092800 ms
Size 4096 optimal GPU: 1.018912 ms

Also see code in 'cs179_lab2.ipynb'

--------------------------------------------------------------------------------
BONUS (+5 points, maximum set score is 100 even with bonus)
--------------------------------------------------------------------------------


Mathematical scripting environments such as Matlab or Python + Numpy often
encourage expressing algorithms in terms of vector operations because they offer
a convenient and performant interface. For instance, one can add 2 n-component
vectors (a and b) in Numpy with c = a + b.

This is often implemented with something like the following code:

void vec_add(float *left, float *right, float *out, int size) {
    for (int i = 0; i < size; i++)
        out[i] = left[i] + right[i];
}

Consider the code

a = x + y + z

where x, y, z are n-component vectors.

One way this could be computed would be

vec_add(x, y, a, n);
vec_add(a, z, a, n);

In what ways is this code (2 calls to vec_add) worse than the following?

for (int i = 0; i < n; i++)
    a[i] = x[i] + y[i] + z[i];

List at least 2 ways (you don't need more than a sentence or two for each way).

1) By calling vec_add twice, we perform 2 for loops of length n, where as the
   second method just does 1 for loop of length n.

2) The computational intensity is lower for the 2 calls to vec_add, since we perform
   one more read and one more write than the second method. 
   More precisely, for the first method, we:
    1 - Read in x and y before adding them
    2 - Write to a
    3 - Read in a and z before adding them
    4 - Write to a
  where as for the second method, we:
    1 - Read in x, y, and z before adding them
    2 - Write to a

